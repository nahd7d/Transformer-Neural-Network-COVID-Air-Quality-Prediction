# Attention Transformer Codes 

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
from tensorflow.keras import layers, regularizers, Model
from tensorflow.keras.optimizers import Adam
import numpy as np
import tensorflow as tf

# Load Data
no2_covid = pd.read_csv('no2_covid.csv')
# Convert categorical variables to numerical
encoder = OneHotEncoder()
encoded_features = encoder.fit_transform(no2_covid[['state']])

# Combine encoded categorical features with numerical features
X = np.concatenate((encoded_features.toarray(), no2_covid[['year', 'week', 'month']].values), axis=1)

# Define target variables
y = no2_covid[['AQI', 'total_adm_all_covid_confirmed_past_7days', 
               'avg_percent_staff_icu_beds_covid', 'COVID-19 Deaths']].values

# Normalize numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Normalize target variables separately
scalers = []
y_scaled = []
for i in range(y.shape[1]):
    scaler = StandardScaler()
    y_scaled.append(scaler.fit_transform(y[:, i].reshape(-1, 1)))
    scalers.append(scaler)

# Combine scaled target variables
y_scaled = np.concatenate(y_scaled, axis=1)

# Cross validation K folds
k_folds = 10
kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)

# Lists to store the performance metrics for each fold
val_losses = []
val_maes = []

# Define the custom model class with L2 regularization
class TemporalMultiScaleSpatialTemporalTransformer(Model):
    def __init__(self, num_heads, d_model, num_layers, num_features, l2_reg=0.01):
        super(TemporalMultiScaleSpatialTemporalTransformer, self).__init__()
        self.encoder = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)
        self.dense = layers.Dense(units=d_model, activation='relu', kernel_regularizer=regularizers.l2(l2_reg))
        self.rnn = layers.LSTM(units=d_model, return_sequences=False)
        self.output_layer = layers.Dense(units=num_features)

    def call(self, inputs, training=True):
        inputs_expanded = tf.expand_dims(inputs, axis=1)
        x = self.encoder(inputs_expanded, inputs_expanded)
        x = tf.squeeze(x, axis=1)
        x = tf.expand_dims(x, axis=1)
        x = self.rnn(x)
        x = self.dense(x)
        return self.output_layer(x)

# Iterate over the folds
for fold, (train_index, val_index) in enumerate(kf.split(X_scaled, y_scaled)):
    print(f"Fold {fold+1}/{k_folds}")
    
    # Split data into train and validation sets for this fold
    X_train_fold, X_val_fold = X_scaled[train_index], X_scaled[val_index]
    y_train_fold, y_val_fold = y_scaled[train_index], y_scaled[val_index]
    
    # Instantiate the model
    num_target_features = 4  # AQI, COVID data, etc.
    model = TemporalMultiScaleSpatialTemporalTransformer(num_heads=8, d_model=64, num_layers=4, 
                                                         num_features=num_target_features, l2_reg=0.01)
    
    # Compile the model
    model.compile(optimizer=Adam(), loss='mse', metrics=['mae'])
    
    # Train the model for the current fold
    history = model.fit(X_train_fold, y_train_fold, epochs=10, batch_size=32, 
                        validation_data=(X_val_fold, y_val_fold), verbose=1)
    
    # Evaluate the model
    val_loss, val_mae = model.evaluate(X_val_fold, y_val_fold, verbose=0)
    val_losses.append(val_loss)
    val_maes.append(val_mae)

# Calculate average validation loss and MAE across all folds
avg_val_loss = sum(val_losses) / len(val_losses)
avg_val_mae = sum(val_maes) / len(val_maes)
print(f"Average validation loss across {k_folds} folds: {avg_val_loss}")
print(f"Average validation MAE across {k_folds} folds: {avg_val_mae}")

# Use the model to predict values for the test data
y_pred = []
y_true = []
for train_index, test_index in kf.split(X_scaled, y_scaled):
    X_train, X_test = X_scaled[train_index], X_scaled[test_index]
    y_train, y_test = y_scaled[train_index], y_scaled[test_index]
    
    # Instantiate and compile the model again
    model = TemporalMultiScaleSpatialTemporalTransformer(num_heads=8, d_model=64, num_layers=4, 
                                                         num_features=num_target_features, l2_reg=0.01)
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])
    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)
    
    # Predict values
    y_pred_fold = model.predict(X_test)
    y_pred.append(y_pred_fold)
    y_true.append(y_test)

# Concatenate predictions and true values
y_pred = np.concatenate(y_pred, axis=0)
y_true = np.concatenate(y_true, axis=0)

# Inverse scaling for predicted and true values
y_pred_original = np.concatenate([scalers[i].inverse_transform(y_pred[:, i].reshape(-1, 1)) 
                                  for i in range(y_pred.shape[1])], axis=1)
y_true_original = np.concatenate([scalers[i].inverse_transform(y_true[:, i].reshape(-1, 1)) 
                                  for i in range(y_true.shape[1])], axis=1)

# Calculate evaluation metrics
mse = mean_squared_error(y_true_original, y_pred_original)
mae = mean_absolute_error(y_true_original, y_pred_original)
rmse = np.sqrt(mse)

# Print the evaluation metrics
print(f"Test MSE: {mse}")
print(f"Test MAE: {mae}")
print(f"Test RMSE: {rmse}")
